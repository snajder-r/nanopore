from meth5.meth5_wrapper import MetH5File

'''
##############################################################################
# Methylation calling using nanopolish
##############################################################################
'''

'''
In preparation for nanopolish methylation calling, raw fast5 entries 
for each read in the fastq files are indexed so they can be accessed more 
efficiently by nanopolish.

The output files will be created within the fastq directory 
(with .index.readdb suffix for each fq file) 
'''
rule nanopolish_index:
    input:
        f5=os.path.join(basedir, 'raw', '{sample}', 'batched', '{batch}'),
        fq=rules.fast5_to_fastq.output
    output: os.path.join(basedir, 'fastq/{sample}/{batch}.%s.index.readdb' % fastq_ending)
    params:
        jobname='npidx_{sample}_{batch}',
        runtime='08:00',
        memusage='2000',
        slots='1',
        misc=''
    shell: '{nanopolish} index -d {input.f5} {input.fq}'

rule nanopolish_index_from_multiplex:
    input:
        f5=os.path.join(basedir, 'raw','multiplexed'),
        fq=rules.fast5_to_fastq.output,
        summary=os.path.join(basedir, 'raw', 'sequence_summary_files.fof.txt')
    output: os.path.join(basedir, 'fastq', '{sample}', '{batch}.%s.index.readdb' % fastq_ending)
    params:
        jobname='npidx_{sample}_{batch}',
        runtime='24:00',
        memusage='16000',
        slots='1',
        misc=''
    shell: '{nanopolish} index -d {input.f5} -f {input.summary} {input.fq}'


rule all_nanopolish_index:
    input: expand(os.path.join(basedir, 'fastq', '{sample}', '{batch}.%s.index.readdb' % fastq_ending), zip_combinator, sample=sb.sample, batch=sb.batch)


'''
Performs methylation calling using nanopolish for one sample and batch, for
one type of methylation call (the "mtype" wildcard). The "mtype" wildcard 
could be one of "cpg", "gpc", or "dam".

The output will be stored in the "met" directory in tsv format.
'''
rule metcall:
    input: 
        fq=rules.fast5_to_fastq.output,
        bam=rules.alignment.output,
        fqidx=rules.nanopolish_index.output
    output: os.path.join(basedir, 'met','{sample}','{batch}_met_{mtype}.tsv')
    params:
        jobname='metcall_{sample}_{batch}',
        runtime='16:00',
        memusage='16000',
        slots='1 -R "span[hosts=1]"',
        misc=''
    shell: '{nanopolish} call-methylation -t 1 -g {reference} -b {input.bam} -r {input.fq} -q {wildcards.mtype} > {output}'

checkpoint all_metcall:
    input: expand(rules.metcall.output, zip2_comb3_combinator, sample=sb.sample, batch=sb.batch, mtype=mettypes)

'''
This merges the nanopolish methylation calls, loads it into a pandas dataframe
and stores it in pickled format.

Note that this job is performed per sample per chromosome. It will save one
outputfile for a sample,chromosome,methylation type combination, in order to 
break up the data and make it easier to load loater.

So while it merges batches, it also splits up chromosomes.
'''
def merge_met_perchrom_input(wildcards):
    return expand(os.path.join(basedir, 
        'met', wildcards.sample, '{batch}_met_%s.tsv' % wildcards.mettype),
        batch=samplebatches(wildcards.sample))

rule merge_met_perchrom:
    input: merge_met_perchrom_input
    output: os.path.join(basedir, 'met_merged', '{sample}', '{chrom}_met_{mettype}.pkl')
    params:
        jobname='mergemet_{sample}_{chrom}',
        runtime='12:00',
        memusage='24000',
        slots='1',
        misc=''
    run:
        pickle_file = output[0]
        chrom = wildcards['chrom']
        sample_met = None
        for f in input:
            for lmet in pd.read_csv(f, sep='\t', header=0, chunksize=1000000,
                               dtype={'chromosome':'category', 
                                      'strand':'category', 
                                      'num_calling_strands':np.uint8, 
                                      'num_motifs':np.uint8}):

                lmet = lmet.loc[lmet.chromosome==chrom].copy()
                if sample_met is None:
                    sample_met = lmet
                else:
                    sample_met = pd.concat((sample_met, lmet))

        sample_met.to_pickle(pickle_file, compression='gzip')

checkpoint all_merge_met_perchrom:
    input: expand(rules.merge_met_perchrom.output, sample=unique_samples, chrom=chroms, mettype=mettypes)


def merge_met_hdf5_input(wildcards):
    return expand(os.path.join(basedir,
        'met', wildcards.sample, '{batch}_met_%s.tsv' % wildcards.mettype),
        batch=samplebatches(wildcards.sample))

rule merge_met_hdf5:
    input: merge_met_hdf5_input
    output: os.path.join(basedir, 'met_merged', '{sample}_{mettype}.h5')
    params:
        jobname='mergemet_{sample}_{mettype}',
        runtime='12:00',
        memusage='8000',
        slots='1',
        misc=''
    run:
        with MetH5File(output[0], 'w') as out_container:
            for tsv_file in input:
                out_container.parse_and_add_nanopolish_file(tsv_file)
        
checkpoint all_merge_met_hdf5:
    input: expand(rules.merge_met_hdf5.output, sample=unique_samples, mettype=mettypes)

    
    

    
    