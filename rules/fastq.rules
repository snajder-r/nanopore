import numpy as np
from modusa.diskusage import check_free_space, compute_total_file_size_gb

'''
##############################################################################
# Extracting FASTQ from basecalled FAST5 files
##############################################################################

This rule will open each basecalled fast5 for a sample and batch, and create
a single fastq file as an outputfile, containing the reads for this sample 
and batch.

The rule depends on the "basecall_id" variable in the config file. This would
typically be "Basecall_1D_000" if there was only a single basecalling 
performed. If there were multiple basecalls (e.g. basecalled with different 
versions of guppy), make sure you specify the correct basecall id in the 
config file.

If individual fast5 files could not be opened or basecalls could not be found,
the entire file will be skipped and a warning written to the log. This is
done because guppy will sometimes file to produce basecalls, and is such a
case we just skip that file and move on.
'''

def fast5_to_fastq_input(wildcards):
    indir = os.path.join(basedir,'raw',wildcards['sample'],'batched', wildcards['batch'])
    if sbf.sbf_filenames is not None:
        for i in range(len(sbf.sbf_filenames)):
            if sbf.sbf_samples[i] == wildcards['sample'] and sbf.sbf_batches[i] == wildcards['batch']:
                yield os.path.join(indir, '%s.fast5'%sbf.sbf_filenames[i])

def compute_ont_mean_q_score(qstring):
    """
    Computing mean quality score as documented here:
    https://labs.epi2me.io/quality-scores/
    """
    qvals = np.array([ord(q) for q in qstring.strip()]) - 33
    return -10 * np.log10(np.sum(np.power(10,(-(qvals) / 10))) / len(qvals))

rule fast5_to_fastq:
    input: fast5_to_fastq_input
    output: os.path.join(basedir, 'fastq', '{sample}', '{batch}.%s' % fastq_ending)
    params:
        jobname='fq_{sample}{batch}',
        runtime='10:00',
        memusage='1000',
        slots='1',
        misc=''
    run:
        # Rudimentary disk space check. Output is estimated to be half of the input size,
        # but we also want to multiply with 2 to be sure.. so this works
        out_path = check_free_space(output[0], compute_total_file_size_gb(input))

        fastq_group='Analyses/{basecall_group}/BaseCalled_template/Fastq'.format(
                        basecall_group=basecall_group)
        passed = 0
        failed = 0
        crit = 0
        with open(out_path,'w') as of:
            for i in range(len(input)):
                try:
                    with h5py.File(input[i],'r') as f:
                        for read in f.keys():
                            try:
                                fq = f[read][fastq_group][()].decode('ascii')
                                fq = fq.split('\n')
                                if read.startswith('read_'):
                                    read = read[5:]
                                fq[0] = f'@{read}'
                                mean_q = compute_ont_mean_q_score(fq[3])
                                if mean_q >= min_read_q:
                                    of.write('\n'.join(fq))
                                    passed+=1
                                else:
                                    failed+=1
                            except KeyError:
                                import traceback
                                print(traceback.format_exc())
                                print(f"Error retreiving {read}/{fastq_group}")
                                crit +=1
                except:
                    print("Error reading h5 file ", input[i])
        print(f"{passed} passed, {failed} QC, {crit} critical error")

checkpoint all_fast5_to_fastq:
    input: expand(rules.fast5_to_fastq.output, zip_combinator, sample=sbf.sbf_samples,  batch=sbf.sbf_batches)


