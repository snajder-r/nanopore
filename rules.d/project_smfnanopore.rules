"""
Dependecies: Need to install 
"""

default_smalljob_params = {"runtime":'1:00', "memusage":'4000', "slots":'1', "misc":''}
default_mediumjob_params = {"runtime":'4:00', "memusage":'8000', "slots":'2', "misc":''}
default_sortjob_params = {"runtime":'16:00', "memusage":'8000', "slots":'2', "misc":''}

if "collapse_strand_frequencies" in globals().keys():
    collapse_strand_frequencies = bool(collapse_strand_frequencies)
else:
    collapse_strand_frequencies = False

def compute_filter_timp_format_input(wc):
    if "_allmtypes_" in wc["basepath"]:
        for mtype in mettypes:
            # dont use format because there could be other wildcards
            basepath = wc["basepath"].replace("_allmtypes_", f"_mtype_{mtype}_")
            yield f"{basepath}_unfiltered_timp_format.tsv.gz"
    else:
        basepath = wc["basepath"]
        yield f"{basepath}_unfiltered_timp_format.tsv.gz"

rule compute_filter_timp_format:
    input: compute_filter_timp_format_input
    output: report= temp("{basepath}_ratefilter_stats.part.json"),
            reads=temp("{basepath}_ratefilter.part.reads.gz"),
    params: **default_smalljob_params,
        jobname = lambda wc: f"filter_timp_format_{Path(wc['basepath']).name}"
    shell: """
           zcat {input} | {python} -m nanopolish_smf.scripts.filter_reads_timp_format -l {min_read_met_rate} -u {max_read_met_rate} -r {output.report} | gzip > {output.reads}
           """

def filter_timp_format_input_reads(wc):
    basepath = wc["basepath"]
    if "_mtype_{mtype}" in basepath:
        basepath = basepath.replace("_mtype_{mtype}", "_allmtypes_")
    return expand(rules.compute_filter_timp_format.output.reads, basepath=basepath)

rule filter_timp_format:
    input:  unfiltered="{basepath}_unfiltered_timp_format.tsv.gz",
            reads=filter_timp_format_input_reads
    output: filtered= temp("{basepath}_ratefiltered_timp_format.tsv.gz"),
    params: **default_smalljob_params,
        jobname = lambda wc: f"filter_timp_format_{Path(wc['basepath']).name}"
    shell: """
           {python} -m nanopolish_smf.scripts.filter_timp_format_by_read -i {input.unfiltered} -r {input.reads} | LC_ALL=C sort -k1,1 -k2,2n | gzip > {output}
           """

rule timp_format_read_stats:
    input: "{basepath}filtered_timp_format.tsv.gz"
    output: temp("{basepath}filtered_read_stats.tsv.gz")
    params: **default_smalljob_params,
        jobname=lambda wc: f"read_stats_{Path(wc['basepath']).name}",
        scriptparams=lambda wc: "-a" if "np_accessibility_" in wc["basepath"] else ""
    shell: """{python} -m nanopolish_smf.scripts.Get_read_stat -v {params.scriptparams} -i {input} | awk 'NR==1; NR>1{{print $0 | "LC_ALL=C sort --parallel={params.slots} -T {scratch_dir} -k1,1d -k2,2n"}}' | gzip > {output}"""

rule timp_format_site_frequencies:
    input: "{basepath}filtered_timp_format.tsv.gz"
    output: temp("{basepath}filtered_site_{collapsed}.tsv.gz")
    params: **default_smalljob_params,
        jobname=lambda wc: f"site_freqs_{Path(wc['basepath']).name}",
        strand_collapse=lambda wc: {"freqs_collapsed": f"-r {reference} -c", "freqs":""}[wc["collapsed"]] # intentionally written so it would throw an error if it were anyhting other than whats in the dict
    shell: """{python} -m nanopolish_smf.scripts.GetFreq_from_bed {params.strand_collapse} -v -i {input} | awk 'NR==1; NR>1{{print $0 | "LC_ALL=C sort --parallel={params.slots} -T {scratch_dir} -k1,1d -k2,2n"}}' | gzip > {output}"""

rule accessibility_to_timp_format:
    input: rules.predict_accessibility.output.prediction
    output: temp(Path(scratch_dir).joinpath('calls_timp_format_batched', "np_accessibility_flank_{minflank}_thres_{llr_thres}_sample_{sample}_batch_{batch}_unfiltered_timp_format.tsv.gz"))
    params:
        **default_smalljob_params,
        jobname = "convert_timp_format_{sample}_{batch}_{minflank}"
    run:
        from nanopolish_smf.timp_format import Converter
        from nanopolish_smf.accessibility import AccessibilityEntry, parse_line
        from pyfaidx import Fasta
        import gzip

        if accessibility_alphabet == "m6a":
            motifs = {"A"}
        elif accessibility_alphabet == "nome":
            motifs = {"CG", "GC", "A"}
        elif accessibility_alphabet == "m5c":
            motifs = {"CG", "GC"}

        with Converter(reference,llr_threshold=float(wildcards["llr_thres"]),motifs=motifs,flank=2,exclude_zero_calls=True) as conv, Fasta(reference) as ref:
            with open(input[0],"r") as in_f, gzip.open(output[0], "tw") as out_f:
                in_f.readline() # header
                for line in in_f:
                    line = line.strip()
                    entry = parse_line(line)
                    if not entry.fwd_strand and ["CG" in motifs or "GC" in motifs]:
                        # Workaround to correct reverse strand calls to point them back to the actual modified base
                        for i, llr in enumerate(entry.llrs.copy()):
                            if llr == 0:
                                continue
                            threemer = ref[entry.chrom][entry.start+i-1:entry.start+i+2].seq
                            if threemer.endswith("CG") and i < len(entry.llrs):
                                entry.llrs[i+1] = entry.llrs[i]
                            if threemer.startswith("GC") and i > 0:
                                entry.llrs[i-1] = entry.llrs[i]

                    line = conv.convert_line(entry)
                    timp_line = conv.format_line(line)
                    if timp_line is not None:
                        out_f.write(f"{timp_line}\n")

rule nanopolish_to_timp_format:
    input: rules.metcall.output
    output: temp(Path(scratch_dir).joinpath('calls_timp_format_batched', "np_metcalls_mtype_{mtype}_thres_{llr_thres}_sample_{sample}_batch_{batch}_unfiltered_timp_format.tsv.gz"))
    params:
        **default_smalljob_params,
        jobname = "convert_timp_format_{sample}_{batch}_{mtype}"
    run:
        from nanopolish_smf.timp_format import Converter
        from nanopolish_smf.accessibility import AccessibilityProfile
        import gzip

        if wildcards["mtype"] == "dam":
            motifs = {"A"}
        elif wildcards["mtype"] == "cpg":
            motifs = {"CG"}
        elif wildcards["mtype"] == "gpc":
            motifs = {"GC"}

        with Converter(reference,llr_threshold=float(wildcards["llr_thres"]),motifs=motifs,flank=2,exclude_zero_calls=True) as conv:
            acc = AccessibilityProfile.read_original_nanopolish_format(input[0])
            with gzip.open(output[0], "tw") as out_f:
                for chrom in acc.chrom_dict:
                    for strand in acc.chrom_dict[chrom]:
                        for entry in acc.chrom_dict[chrom][strand]:
                            line_dict = conv.convert_line(entry)
                            timp_line = conv.format_line(line_dict)
                            if timp_line is not None:
                                out_f.write(f"{timp_line}\n")

if "nanonome" in globals().keys():
    rule nanonome_to_timp_format:
        input: rules.nanonome.output
        output: temp(Path(scratch_dir).joinpath('calls_timp_format_batched', "nanonome_thres_{llr_thres}_sample_{sample}_batch_{batch}_unfiltered_timp_format.tsv.gz"))
        params:
            **default_smalljob_params,
            jobname = "convert_timp_format_{sample}_{batch}"
        run:
            from nanopolish_smf.timp_format import Converter
            from nanopolish_smf.accessibility import AccessibilityProfile
            import gzip
            with Converter(reference,llr_threshold=float(wildcards["llr_thres"]),motifs={"CG", "GC"},flank=2,exclude_zero_calls=True) as conv:
                acc = AccessibilityProfile.read_original_nanopolish_format(input[0])
                with gzip.open(output[0], "tw") as out_f:
                    for chrom in acc.chrom_dict:
                        for strand in acc.chrom_dict[chrom]:
                            for entry in acc.chrom_dict[chrom][strand]:
                                line_dict = conv.convert_line(entry)
                                timp_line = conv.format_line(line_dict)
                                if timp_line is not None:
                                    out_f.write(f"{timp_line}\n")
else:
    """Dummy rule"""
    rule nanonome_to_timp_format:
        output: []

    llr_threshold_nanonome = []


rule bonito_timp_format:
    input: bam=rules.bonito_basecall_metcall.output.sorted,
           bai=rules.bonito_basecall_metcall.output.sorted+".bai"
    output: temp(Path(scratch_dir).joinpath('calls_timp_format_batched',"bonito_metcalls_thres_{llr_thres}_sample_{sample}_batch_{batch}_unfiltered_timp_format.tsv.gz"))
    params:
        **default_smalljob_params,
        jobname="convert_bonito_timp_format_{sample}_{batch}"
    run:
        from modbampy import ModBam
        import numpy as np
        import gzip
        from nanopolish_smf.timp_format import Converter
        from nanopolish_smf.accessibility import AccessibilityEntry
        from nanopolish_smf.math import p_to_llr

        if "remora_motifs" in globals().keys():
            motifs = set(remora_motifs)
        else:
            motifs = {"CG"}

        with Converter(reference, exclude_zero_calls=True, llr_threshold=float(wildcards["llr_thres"]), motifs=motifs) as converter, gzip.open(output[0], "wt") as out_f:
            for chrom in converter.ref.keys():
                for read in ModBam(input.bam).reads(chrom,0,len(converter.ref[chrom])):
                    llrs = np.zeros((read.reference_end - read.reference_start))
                    for pos_mod in read.mod_sites:
                        rpos = pos_mod.rpos
                        if rpos < 0:
                            continue
                        p = float(pos_mod.qual) / 256
                        llrs[rpos - read.reference_start] = p_to_llr(np.clip(p, 0.000001, 0.999999))

                    read_entry = AccessibilityEntry(chrom,read.reference_start,read.reference_end,
                        not read.is_reverse,read.query_name,llrs)

                    timp_line = converter.format_line(converter.convert_line(read_entry))
                    if timp_line is None:
                        continue
                    out_f.write(f"{timp_line}\n")

def merge_timp_format_input(wildcards):
    source = wildcards["source"]
    how = wildcards["how"]
    for sample, batch in zip(sbf.sb_samples, sbf.sb_batches):
        suffix = f"sample_{sample}_batch_{batch}_{how}filtered_timp_format.tsv.gz"
        filtered = Path(scratch_dir).joinpath('calls_timp_format_batched', f"{source}_{suffix}")
        yield filtered

rule merge_timp_format:
    input: merge_timp_format_input
    output: Path(basedir).joinpath('calls_timp_format', "{source}_{how}filtered_calls.tsv.bgz")
    params:
        **default_sortjob_params,
        jobname = "merge_timp_format"
    shell: """
               {tabix_load_hook}
               zcat {input} | LC_ALL=C sort --parallel={params.slots} -T {scratch_dir} -k1,1 -k2,2n | bgzip > {output};
               sleep 10 # napping to make sure index is not too new
               tabix -p bed  {output}
            """

def merge_timp_format_read_stats_input(wildcards, statstype=None):
    source = wildcards["source"]
    how = wildcards["how"]
    for sample, batch in zip(sbf.sb_samples, sbf.sb_batches):
        suffix = f"sample_{sample}_batch_{batch}_{how}filtered_{statstype}.tsv.gz"
        filtered = Path(scratch_dir).joinpath('calls_timp_format_batched', f"{source}_{suffix}")
        yield filtered

rule merge_timp_format_read_stats:
    input: lambda wc: merge_timp_format_read_stats_input(wc, statstype="read_stats")
    output: part=Path(basedir).joinpath('calls_timp_format', "{source}_{how}filtered.read_stats.tsv.bgz")
    params:
        **default_sortjob_params,
        jobname = "merge_timp_format_read_stats_{source}_{how}filtered"
    shell: """
               {tabix_load_hook}
               {python} -m nanopolish_smf.scripts.merge_presorted_bed -i {input} -s 1 | bgzip > {output.part};
               sleep 10 # napping to make sure index is not too new
               tabix -p bed -S 1 {output.part};
            """

rule merge_timp_format_site_freqs:
    input: lambda wc: merge_timp_format_read_stats_input(wc, statstype="site_" + wc["collapsed"])
    output: part=Path(basedir).joinpath('calls_timp_format', "{source}_{how}filtered.site_{collapsed}.tsv.bgz")
    params:
        **default_sortjob_params,
        jobname = "merge_timp_format_site_{collapsed}_{source}_{how}filtered"
    shell: """
               {tabix_load_hook}
               {python} -m nanopolish_smf.scripts.merge_presorted_bed -i {input} -s 1 | {python} -m nanopolish_smf.scripts.aggregate_met_counts_frequencies -s 1 | bgzip > {output.part};
               sleep 10 # napping to make sure index is not too new
               tabix -p bed -S 1 {output.part};
            """

def merge_timp_format_filter_stats_input(wildcards):
    source = wildcards["source"]
    for sample, batch in zip(sbf.sb_samples, sbf.sb_batches):
        suffix = f"sample_{sample}_batch_{batch}_ratefilter_stats.part.json"
        part = Path(scratch_dir).joinpath('calls_timp_format_batched', f"{source}_{suffix}")
        yield part

rule merge_timp_format_filter_stats:
    input: merge_timp_format_filter_stats_input
    output: temp(Path(basedir).joinpath('calls_timp_format', "{source}_ratefilter_stats.json"))
    params:
        **default_sortjob_params,
        jobname = "merge_timp_format_filter_stats"
    run:
        import json
        total_stats = {}
        for file in input:
            with open(file, "rt") as f:
                part_dict = json.load(f)
                for k,v in part_dict.items():
                    total_stats[k] = total_stats.get(k, 0) + v
        with open(output[0], "wt") as f:
            json.dump(total_stats, f)

def all_smf_timpformat_input(timp_rule, calls=True, call_stats=True, filter_stats=True, **wildcards):
    if len(timp_rule.output) == 0:
        return
    partfile = Path(timp_rule.output[0]).name
    source = partfile.replace("_sample_{sample}_batch_{batch}_unfiltered_timp_format.tsv.gz", "")
    sources = expand(source,**wildcards)
    for filter_how in ["un", "rate"]:
        if calls:
            for source in sources:
                yield rules.merge_timp_format.output[0].format(source=source, how=filter_how)
        if call_stats:
            for source in sources:
                yield rules.merge_timp_format_read_stats.output.part.format(source=source, how=filter_how)
                if collapse_strand_frequencies:
                    yield rules.merge_timp_format_site_freqs.output.part.format(source=source, how=filter_how, collapsed="freqs_collapsed")
                else:
                    yield rules.merge_timp_format_site_freqs.output.part.format(source=source,how=filter_how, collapsed="freqs")


    # Filter stats
    if filter_stats:
        for source in sources:
            source = source.replace("_mtype_cpg_", "_allmtypes_").replace("_mtype_gpc_", "_allmtypes_").replace("_mtype_dam_", "_allmtypes_")
            yield rules.merge_timp_format_filter_stats.output[0].format(source=source)


rule smf_timpformat_report:
    input:
        np_accessibility=all_smf_timpformat_input(rules.accessibility_to_timp_format, calls=False, call_stats=False, filter_stats=True, minflank=all_flanking_options, llr_thres=llr_threshold_accessibility),
        nanonome=all_smf_timpformat_input(rules.nanonome_to_timp_format, calls=False, call_stats=False, filter_stats=True, llr_thres=llr_threshold_nanonome),
        nanopolish=all_smf_timpformat_input(rules.nanopolish_to_timp_format, calls=False, call_stats=False, filter_stats=True, mtype=mettypes, llr_thres=llr_threshold_nanopolish),
        bonito=all_smf_timpformat_input(rules.bonito_timp_format, calls=False, call_stats=False, filter_stats=True,  llr_thres=llr_threshold_bonito)
    output: f"{basedir}/report/smf/filtering_stats.txt"
    params:
        **default_sortjob_params,
        jobname="smf_report"
    run:
        import json
        import pandas as pd
        from pathlib import Path

        rows = {}
        for caller in input.keys():
            for file in input[caller]:
                file = Path(str(file))
                source = file.name.replace("_ratefilter_stats.json", "")
                with open(file) as f:
                    stats = json.load(f)
                    rows[source] = stats
        report = pd.DataFrame(rows).transpose()
        report.to_csv(output[0], sep="\t")


rule convert_np_accessibility_to_modbam:
    input: metcall = rules.predict_accessibility.output.prediction,
           bam = rules.alignment.output[0],
           bai = rules.alignment.output[0] + ".bai"
    output: bam = temp(Path(predict_accessibility_outdir(), "accessibility_modbam.bam"))
    params: jobname="convert_np_acc_modbam_{sample}_{batch}",
            **default_smalljob_params
    run:
        from nanopolish_smf.modbam import ModBamWriter
        from nanopolish_smf.accessibility import AccessibilityProfile

        if accessibility_alphabet == "m6a":
            modified_bases = {"x":"A"}
        elif accessibility_alphabet == "nome":
            modified_bases = {"z": "C", "x": "A"}
        elif accessibility_alphabet == "m5c":
            modified_bases = {"z": "C"}

        acc = AccessibilityProfile.read(input.metcall)
        with ModBamWriter(input.bam, output.bam, modified_bases=modified_bases) as writer:
            writer.convert(acc)

rule sort_np_accessibility_modbam:
    input: bam = rules.convert_np_accessibility_to_modbam.output.bam
    output: bam = temp(Path(predict_accessibility_outdir(), "accessibility_modbam.sorted.bam"))
    params: jobname="sort_modbam_{sample}_{batch}",
            **default_mediumjob_params
    shell: "{samtools} sort {input.bam} > {output.bam}"

rule prepare_merge_np_accessibility_modbam:
    input: lambda wc: expand(rules.sort_np_accessibility_modbam.output.bam, sample=wc["sample"], minflank=wc["minflank"], batch=samplebatches(wc["sample"]))
    output: filelist = temp(Path(basedir).joinpath('nanopolish_accessibility', '{sample}', 'flank_{minflank}', "accessibility_modbam_merging_filelist.txt"))
    params: **default_smalljob_params, jobname="prepare_merge_np_acc_modbam"
    shell: 'echo {input} | sed \'s/ /\\n/g\' > {output.filelist}'

rule merge_np_accessibility_modbam:
    input: filelist = rules.prepare_merge_np_accessibility_modbam.output.filelist
    output: bam = Path(basedir).joinpath('nanopolish_accessibility', '{sample}', 'flank_{minflank}', "accessibility_modbam.bam")
    params:
        jobname='mergemodbam_{sample}_{minflank}',
        runtime='24:00',
        memusage='16000',
        slots='1',
        misc=''
    shell:
        "{samtools} merge -b {input.filelist} {output.bam}"

rule all_merge_np_accessibility_modbam:
    input: expand(rules.merge_np_accessibility_modbam.output.bam, sample=unique_samples, minflank=all_flanking_options)

rule all_smf_timpformat:
    input:
        all_smf_timpformat_input(rules.accessibility_to_timp_format, minflank=all_flanking_options, llr_thres=llr_threshold_accessibility),
        all_smf_timpformat_input(rules.nanonome_to_timp_format, llr_thres=llr_threshold_nanonome),
        all_smf_timpformat_input(rules.nanopolish_to_timp_format, mtype=mettypes, llr_thres=llr_threshold_nanopolish),
        all_smf_timpformat_input(rules.bonito_timp_format, llr_thres=llr_threshold_bonito),
        rules.smf_timpformat_report.output,

rule all_smf_nanopore:
    input:
        expand(rules.mergebams.output, sample=unique_samples),
        rules.smf_timpformat_report.output,
        #expand(rules.pycoqc_report.output, sample=unique_samples),
        all_smf_timpformat_input(rules.accessibility_to_timp_format, minflank=all_flanking_options, llr_thres=llr_threshold_accessibility),
        all_smf_timpformat_input(rules.nanonome_to_timp_format, llr_thres=llr_threshold_nanonome),
        all_smf_timpformat_input(rules.nanopolish_to_timp_format, mtype=mettypes, llr_thres=llr_threshold_nanopolish),
        all_smf_timpformat_input(rules.bonito_timp_format, llr_thres=llr_threshold_bonito)

rule smf_nanopore_remora_only:
    input:
        expand(rules.mergebams.output, sample=unique_samples),
        all_smf_timpformat_input(rules.bonito_timp_format, llr_thres=llr_threshold_bonito)

rule smf_nanopore_accessibility_only:
    input:
        expand(rules.mergebams.output, sample=unique_samples),
        all_smf_timpformat_input(rules.accessibility_to_timp_format, llr_thres=llr_threshold_accessibility, minflank=all_flanking_options)