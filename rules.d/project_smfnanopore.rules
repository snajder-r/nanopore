"""
Dependecies: Need to install 
"""

default_smalljob_params = {"runtime":'1:00', "memusage":'4000', "slots":'1', "misc":''}
default_mediumjob_params = {"runtime":'4:00', "memusage":'8000', "slots":'2', "misc":''}
default_sortjob_params = {"runtime":'16:00', "memusage":'8000', "slots":'2', "misc":''}

def compute_filter_timp_format_input(wc):
    if "_allmtypes_" in wc["basepath"]:
        for mtype in ["cpg", "gpc"]:
            # dont use format because there could be other wildcards
            basepath = wc["basepath"].replace("_allmtypes_", f"_mtype_{mtype}_")
            yield f"{basepath}_unfiltered_timp_format.tsv.gz"
    else:
        basepath = wc["basepath"]
        yield f"{basepath}_unfiltered_timp_format.tsv.gz"

rule compute_filter_timp_format:
    input: compute_filter_timp_format_input
    output: report= temp("{basepath}_ratefilter_stats.part.json"),
            reads=temp("{basepath}_ratefilter.part.reads.gz"),
    params: **default_smalljob_params,
        jobname = lambda wc: f"filter_timp_format_{Path(wc['basepath']).name}"
    shell: """
           zcat {input} | {python} -m nanopolish_smf.scripts.filter_reads_timp_format -l {min_read_met_rate} -u {max_read_met_rate} -r {output.report} | gzip > {output.reads}
           """

def filter_timp_format_input_reads(wc):
    basepath = wc["basepath"]
    if "_mtype_{mtype}" in basepath:
        basepath = basepath.replace("_mtype_{mtype}", "_allmtypes_")
    return expand(rules.compute_filter_timp_format.output.reads, basepath=basepath)

rule filter_timp_format:
    input:  unfiltered="{basepath}_unfiltered_timp_format.tsv.gz",
            reads=filter_timp_format_input_reads
    output: filtered= temp("{basepath}_ratefiltered_timp_format.tsv.gz"),
    params: **default_smalljob_params,
        jobname = lambda wc: f"filter_timp_format_{Path(wc['basepath']).name}"
    shell: """
           {python} -m nanopolish_smf.scripts.filter_timp_format_by_read -i {input.unfiltered} -r {input.reads} | LC_ALL=C sort -k1,1 -k2,2n | gzip > {output}
           """

rule timp_format_read_stats:
    input: "{basepath}filtered_timp_format.tsv.gz"
    output: temp("{basepath}filtered_read_stats.tsv.gz")
    params: **default_smalljob_params,
        jobname=lambda wc: f"read_stats_{Path(wc['basepath']).name}",
        scriptparams=lambda wc: "-a" if "np_accessibility_" in wc["basepath"] else ""
    shell: """{python} /g/krebs/boulanger/Scripts/PythonScripts/Get_read_stat.py -v {params.scriptparams} -i {input} | awk 'NR==1; NR>1{{print $0 | "LC_ALL=C sort --parallel={params.slots} -T {scratch_dir} -k1,1d -k2,2n"}}' | gzip > {output}"""

rule timp_format_read_frequencies:
    input: "{basepath}filtered_timp_format.tsv.gz"
    output: temp("{basepath}filtered_site_freqs.tsv.gz")
    params: **default_smalljob_params,
        jobname=lambda wc: f"site_freqs_{Path(wc['basepath']).name}",
    shell: """{python} /g/krebs/boulanger/Scripts/PythonScripts/GetFreq_from_bed.py -v -i {input} | awk 'NR==1; NR>1{{print $0 | "LC_ALL=C sort --parallel={params.slots} -T {scratch_dir} -k1,1d -k2,2n"}}' | gzip > {output}"""

rule accessibility_to_timp_format:
    input: rules.predict_accessibility.output.prediction
    output: temp(Path(basedir).joinpath('calls_timp_format', "tmp", "np_accessibility_flank_{minflank}_thres_{llr_thres}_sample_{sample}_batch_{batch}_unfiltered_timp_format.tsv.gz"))
    params:
        **default_smalljob_params,
        jobname = "convert_timp_format_{sample}_{batch}_{minflank}"
    run:
        from nanopolish_smf.timp_format import Converter
        with Converter(reference, llr_threshold=float(wildcards["llr_thres"]), flank=2) as conv:
            conv.convert_file(str(input), str(output))

rule nanopolish_to_timp_format:
    input: rules.metcall.output
    output: temp(Path(basedir).joinpath('calls_timp_format', "tmp", "np_metcalls_mtype_{mtype}_thres_{llr_thres}_sample_{sample}_batch_{batch}_unfiltered_timp_format.tsv.gz"))
    params:
        **default_smalljob_params,
        jobname = "convert_timp_format_{sample}_{batch}_{mtype}"
    shell:
        "{python} {nanopolish_timp_conversion_script} -c {wildcards.llr_thres} -i {input} -q {wildcards.mtype} -g {reference} | gzip > {output}"

rule nanonome_to_timp_format:
    input: rules.nanonome.output
    output: temp(Path(basedir).joinpath('calls_timp_format', "tmp", "nanonome_thres_{llr_thres}_sample_{sample}_batch_{batch}_unfiltered_timp_format.tsv.gz"))
    params:
        **default_smalljob_params,
        jobname = "convert_timp_format_{sample}_{batch}"
    shell:
        "{python} {nanopolish_timp_conversion_script} -c {wildcards.llr_thres} -i {input} -q cpggpc -g {reference} |gzip> {output}"

rule bonito_timp_format:
    input: bam=rules.bonito_basecall_metcall.output.sorted,
           bai=rules.bonito_basecall_metcall.output.sorted+".bai"
    output: temp(Path(basedir).joinpath("calls_timp_format", "tmp", "bonito_metcalls_thres_{llr_thres}_sample_{sample}_batch_{batch}_unfiltered_timp_format.tsv.gz"))
    params:
        **default_smalljob_params,
        jobname="convert_bonito_timp_format_{sample}_{batch}"
    run:
        from modbampy import ModBam
        import numpy as np
        import gzip
        from nanopolish_smf.timp_format import Converter
        from nanopolish_smf.accessibility import AccessibilityEntry

        def p_to_llr(p, prior=0.5):
            """
            Converts the posterior probability p(a|x) into a log-likelihood ratio
            log(p(x|a)/p(x|~a)) given a prior pa(a)
            """
            return -np.log(prior * (1 - p) / (p * (1 - prior)))

        with Converter(reference, llr_threshold=float(wildcards["llr_thres"]), motifs={"CG"}) as converter, gzip.open(output[0], "wt") as out_f:
            for chrom in converter.ref.keys():
                for read in ModBam(input.bam,chrom,0,len(converter.ref[chrom])).reads():
                    llrs = np.zeros((read.reference_end - read.reference_start))
                    for pos_mod in read.mod_sites:
                        rpos = pos_mod.rpos
                        if rpos < 0:
                            continue
                        if read.is_reverse:
                            rpos -= 1  # otherwise it points at the G
                        p = float(pos_mod.qual) / 256
                        llrs[rpos - read.reference_start] = p_to_llr(np.clip(p, 0.000001, 0.999999))

                    read_entry = AccessibilityEntry(chrom,read.reference_start,read.reference_end,
                        True,read.query_name,llrs)

                    timp_line = converter.format_line(converter.convert_line(read_entry))
                    if timp_line is None:
                        continue
                    out_f.write(f"{timp_line}\n")



def merge_timp_format_input(wildcards):
    source = wildcards["source"]
    how = wildcards["how"]
    for sample, batch in zip(sbf.sb_samples, sbf.sb_batches):
        suffix = f"sample_{sample}_batch_{batch}_{how}filtered_timp_format.tsv.gz"
        filtered = Path(basedir).joinpath("calls_timp_format", "tmp", f"{source}_{suffix}")
        yield filtered

rule merge_timp_format:
    input: merge_timp_format_input
    output: Path(basedir).joinpath('calls_timp_format', "{source}_{how}filtered_calls.tsv.bgz")
    params:
        **default_sortjob_params,
        jobname = "merge_timp_format"
    shell: """
               {tabix_load_hook}
               zcat {input} | LC_ALL=C sort --parallel={params.slots} -T {scratch_dir} -k1,1 -k2,2n | bgzip > {output};
               sleep 10 # napping to make sure index is not too new
               tabix -p bed  {output}
            """

def merge_timp_format_read_stats_input(wildcards, statstype=None):
    source = wildcards["source"]
    how = wildcards["how"]
    for sample, batch in zip(sbf.sb_samples, sbf.sb_batches):
        suffix = f"sample_{sample}_batch_{batch}_{how}filtered_{statstype}.tsv.gz"
        filtered = Path(basedir).joinpath("calls_timp_format", "tmp", f"{source}_{suffix}")
        yield filtered

rule merge_timp_format_read_stats:
    input: lambda wc: merge_timp_format_read_stats_input(wc, statstype="read_stats")
    output: part=Path(basedir).joinpath('calls_timp_format', "{source}_{how}filtered.read_stats.tsv.bgz")
    params:
        **default_sortjob_params,
        jobname = "merge_timp_format_read_stats_{source}_{how}filtered"
    shell: """
               {tabix_load_hook}
               {python} -m nanopolish_smf.scripts.merge_presorted_bed -i {input} -s 1 | bgzip > {output.part};
               sleep 10 # napping to make sure index is not too new
               tabix -p bed -S 1 {output.part};
            """

rule merge_timp_format_site_freqs:
    input: lambda wc: merge_timp_format_read_stats_input(wc, statstype="site_freqs")
    output: part=Path(basedir).joinpath('calls_timp_format', "{source}_{how}filtered.site_freqs.tsv.bgz")
    params:
        **default_sortjob_params,
        jobname = "merge_timp_format_site_freqs_{source}_{how}filtered"
    shell: """
               {tabix_load_hook}
               {python} -m nanopolish_smf.scripts.merge_presorted_bed -i {input} -s 1 | {python} -m nanopolish_smf.scripts.aggregate_met_counts_frequencies -s 1 | bgzip > {output.part};
               sleep 10 # napping to make sure index is not too new
               tabix -p bed -S 1 {output.part};
            """

def merge_timp_format_filter_stats_input(wildcards):
    source = wildcards["source"]
    for sample, batch in zip(sbf.sb_samples, sbf.sb_batches):
        suffix = f"sample_{sample}_batch_{batch}_ratefilter_stats.part.json"
        part = Path(basedir).joinpath("calls_timp_format", "tmp", f"{source}_{suffix}")
        yield part

rule merge_timp_format_filter_stats:
    input: merge_timp_format_filter_stats_input
    output: temp(Path(basedir).joinpath('calls_timp_format', "{source}_ratefilter_stats.json"))
    params:
        **default_sortjob_params,
        jobname = "merge_timp_format_filter_stats"
    run:
        import json
        total_stats = {}
        for file in input:
            with open(file, "rt") as f:
                part_dict = json.load(f)
                for k,v in part_dict.items():
                    total_stats[k] = total_stats.get(k, 0) + v
        with open(output[0], "wt") as f:
            json.dump(total_stats, f)

def all_smf_timpformat_input(timp_rule, calls=True, call_stats=True, filter_stats=True, **wildcards):
    partfile = Path(timp_rule.output[0]).name
    source = partfile.replace("_sample_{sample}_batch_{batch}_unfiltered_timp_format.tsv.gz", "")
    sources = expand(source,**wildcards)
    for filter_how in ["un", "rate"]:
        if calls:
            for source in sources:
                yield rules.merge_timp_format.output[0].format(source=source, how=filter_how)
        if call_stats:
            for source in sources:
                yield rules.merge_timp_format_read_stats.output.part.format(source=source, how=filter_how)
                yield rules.merge_timp_format_site_freqs.output.part.format(source=source, how=filter_how)

    # Filter stats
    if filter_stats:
        for source in sources:
            source = source.replace("_mtype_cpg_", "_allmtypes_").replace("_mtype_gpc_", "_allmtypes_")
            yield rules.merge_timp_format_filter_stats.output[0].format(source=source)


rule smf_timpformat_report:
    input:
        np_accessibility=all_smf_timpformat_input(rules.accessibility_to_timp_format, calls=False, call_stats=False, filter_stats=True, minflank=all_flanking_options, llr_thres=llr_threshold_accessibility),
        nanonome=all_smf_timpformat_input(rules.nanonome_to_timp_format, calls=False, call_stats=False, filter_stats=True, llr_thres=llr_threshold_nanonome),
        nanopolish=all_smf_timpformat_input(rules.nanopolish_to_timp_format, calls=False, call_stats=False, filter_stats=True, mtype=["cpg","gpc"], llr_thres=llr_threshold_nanopolish),
        bonito=all_smf_timpformat_input(rules.bonito_timp_format, calls=False, call_stats=False, filter_stats=True,  llr_thres=llr_threshold_bonito)
    output: f"{basedir}/report/smf/filtering_stats.txt"
    params:
        **default_sortjob_params,
        jobname="smf_report"
    run:
        import json
        import pandas as pd
        from pathlib import Path

        rows = {}
        for caller in input.keys():
            for file in input[caller]:
                file = Path(str(file))
                source = file.name.replace("_ratefilter_stats.json", "")
                with open(file) as f:
                    stats = json.load(f)
                    rows[source] = stats
        report = pd.DataFrame(rows).transpose()
        report.to_csv(output[0], sep="\t")

rule all_smf_timpformat:
    input:
        all_smf_timpformat_input(rules.accessibility_to_timp_format, minflank=all_flanking_options, llr_thres=llr_threshold_accessibility),
        all_smf_timpformat_input(rules.nanonome_to_timp_format, llr_thres=llr_threshold_nanonome),
        all_smf_timpformat_input(rules.nanopolish_to_timp_format, mtype=["cpg","gpc"], llr_thres=llr_threshold_nanopolish),
        all_smf_timpformat_input(rules.bonito_timp_format, llr_thres=llr_threshold_bonito),
        rules.smf_timpformat_report.output,

rule all_smf_nanopore:
    input:
        expand(rules.mergebams.output, sample=unique_samples),
        rules.smf_timpformat_report.output,
        #expand(rules.pycoqc_report.output, sample=unique_samples),
        all_smf_timpformat_input(rules.accessibility_to_timp_format, minflank=all_flanking_options, llr_thres=llr_threshold_accessibility),
        all_smf_timpformat_input(rules.nanonome_to_timp_format, llr_thres=llr_threshold_nanonome),
        all_smf_timpformat_input(rules.nanopolish_to_timp_format, mtype=["cpg","gpc"], llr_thres=llr_threshold_nanopolish),
        all_smf_timpformat_input(rules.bonito_timp_format, llr_thres=llr_threshold_bonito)
